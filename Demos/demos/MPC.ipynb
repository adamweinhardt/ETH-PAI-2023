{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Uniform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns \n",
    "import gpytorch \n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "import IPython\n",
    "# If in your browser the figures are not nicely vizualized, change the following line.\n",
    "rcParams['font.size'] = 20\n",
    "rcParams['figure.figsize'] = (20, 8)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layout = ipywidgets.Layout(width='auto', height='40px')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rllib.environment import SystemEnvironment\n",
    "from rllib.environment.systems.inverted_pendulum import InvertedPendulum\n",
    "from rllib.agent import MPCAgent\n",
    "from rllib.util.rollout import rollout_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC: Planning with a known model for a fixed horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "from rllib.value_function.abstract_value_function import AbstractValueFunction\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.distributions import MultivariateNormal\n",
    "from rllib.model import AbstractModel\n",
    "from rllib.reward.utilities import tolerance\n",
    "\n",
    "\n",
    "class StateTransform(nn.Module):\n",
    "    \"\"\"Transform pendulum states to cos, sin, angular_velocity.\"\"\"\n",
    "\n",
    "    extra_dim = 1\n",
    "\n",
    "    def forward(self, states_):\n",
    "        \"\"\"Transform state before applying function approximation.\"\"\"\n",
    "        angle, angular_velocity = torch.split(states_, 1, dim=-1)\n",
    "        states_ = torch.cat(\n",
    "            (torch.cos(angle), torch.sin(angle), angular_velocity), dim=-1\n",
    "        )\n",
    "        return states_\n",
    "\n",
    "    def inverse(self, states_):\n",
    "        \"\"\"Inverse transformation of states.\"\"\"\n",
    "        cos, sin, angular_velocity = torch.split(states_, 1, dim=-1)\n",
    "        angle = torch.atan2(sin, cos)\n",
    "        states_ = torch.cat((angle, angular_velocity), dim=-1)\n",
    "        return states_\n",
    "\n",
    "\n",
    "def large_state_termination(state, action, next_state=None):\n",
    "    \"\"\"Termination condition for environment.\"\"\"\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state)\n",
    "    if not isinstance(action, torch.Tensor):\n",
    "        action = torch.tensor(action)\n",
    "\n",
    "    done = torch.any(torch.abs(state) > 200, dim=-1) | torch.any(\n",
    "        torch.abs(action) > 200, dim=-1\n",
    "    )\n",
    "    return (\n",
    "        torch.zeros(*done.shape, 2)\n",
    "        .scatter_(dim=-1, index=(~done).long().unsqueeze(-1), value=-float(\"inf\"))\n",
    "        .squeeze(-1)\n",
    "    )\n",
    "\n",
    "\n",
    "class PendulumSparseReward(AbstractModel):\n",
    "    \"\"\"Reward for Inverted Pendulum.\"\"\"\n",
    "\n",
    "    def __init__(self, action_cost=0, radius=0.05):\n",
    "        super().__init__(dim_state=(2,), dim_action=(1,), model_kind=\"rewards\", dim_reward=(1,))\n",
    "        self.action_cost = action_cost\n",
    "        self.reward_offset = 0\n",
    "        self.radius = 0.05\n",
    "\n",
    "    def forward(self, state, action, next_state):\n",
    "        \"\"\"See `abstract_reward.forward'.\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.get_default_dtype())\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action, dtype=torch.get_default_dtype())\n",
    "\n",
    "        cos_angle = torch.cos(state[..., 0])\n",
    "        velocity = state[..., 1]\n",
    "\n",
    "        angle_tolerance = tolerance(cos_angle, lower=1 - self.radius, upper=1.0, margin=0.1)\n",
    "        velocity_tolerance = tolerance(velocity, lower=-0.5, upper=0.5, margin=0.5)\n",
    "        state_cost = angle_tolerance * velocity_tolerance\n",
    "\n",
    "        action_tolerance = tolerance(action[..., 0], lower=-0.1, upper=0.1, margin=0.1)\n",
    "        action_cost = self.action_cost * (action_tolerance - 1)\n",
    "\n",
    "        cost = state_cost + action_cost\n",
    "\n",
    "        return cost.unsqueeze(-1), torch.zeros(1)\n",
    "\n",
    "\n",
    "class PendulumDenseReward(AbstractModel):\n",
    "    \"\"\"Reward for Inverted Pendulum.\"\"\"\n",
    "\n",
    "    def __init__(self, action_cost=0.0):\n",
    "        super().__init__(dim_state=(2,), dim_action=(1,), model_kind=\"rewards\", dim_reward=(1,))\n",
    "        self.action_cost = action_cost\n",
    "        self.reward_offset = 0\n",
    "\n",
    "    def forward(self, state, action, next_state):\n",
    "        \"\"\"See `abstract_reward.forward'.\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.get_default_dtype())\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action, dtype=torch.get_default_dtype())\n",
    "\n",
    "        cos_angle = 1 - torch.cos(state[..., 0])\n",
    "        state_cost = cos_angle ** 2\n",
    "        action_cost = self.action_cost * (action ** 2).sum(-1)\n",
    "\n",
    "        return -(action_cost + state_cost).unsqueeze(-1), torch.tensor(0.0)\n",
    "\n",
    "\n",
    "class PendulumModel(AbstractModel):\n",
    "    \"\"\"Pendulum Model.\n",
    "\n",
    "    Torch implementation of a pendulum model using euler forwards integration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, mass, length, friction, step_size=1 / 80, noise: MultivariateNormal = None\n",
    "    ):\n",
    "        super().__init__(dim_state=(2,), dim_action=(1,))\n",
    "        self.mass = mass\n",
    "        self.length = length\n",
    "        self.friction = friction\n",
    "        self.step_size = step_size\n",
    "        self.noise = noise\n",
    "\n",
    "    def forward(self, state, action,next_state):\n",
    "        \"\"\"Get next-state distribution.\"\"\"\n",
    "        # Physical dynamics\n",
    "        action = torch.clamp(action, -1.0, 1.0)\n",
    "        mass = self.mass\n",
    "        gravity = 9.81\n",
    "        length = self.length\n",
    "        friction = self.friction\n",
    "        inertia = mass * length ** 2\n",
    "        dt = self.step_size\n",
    "\n",
    "        angle, angular_velocity = torch.split(state, 1, dim=-1)\n",
    "        for _ in range(1):\n",
    "            x_ddot = (\n",
    "                (gravity / length) * torch.sin(angle)\n",
    "                + action * (1 / inertia)\n",
    "                - (friction / inertia) * angular_velocity\n",
    "            )\n",
    "\n",
    "            angle = angle + dt * angular_velocity\n",
    "            angular_velocity = angular_velocity + dt * x_ddot\n",
    "\n",
    "        next_state = torch.cat((angle, angular_velocity), dim=-1)\n",
    "\n",
    "        if self.noise is None:\n",
    "            return next_state, torch.zeros(1)\n",
    "        else:\n",
    "            return next_state + self.noise.mean, self.noise.covariance_matrix\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class PendulumReturnToGo(AbstractValueFunction):\n",
    "    def __init__(self):\n",
    "        super().__init__(dim_state = 2, dim_action = 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        angle, velocity = x[..., 0], x[..., 1]\n",
    "        cos_angle = torch.cos(angle)\n",
    "        out = torch.zeros_like(angle)\n",
    "        \n",
    "        idx = cos_angle < np.cos(np.pi / 6)\n",
    "        not_idx = cos_angle >= np.cos(np.pi / 6)\n",
    "        \n",
    "        out = 400 * cos_angle\n",
    "        return 400 * cos_angle.unsqueeze(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {},
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306d562a72254e0e96fe34639c26d8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='sparse_reward'), Checkbox(value=False, description='râ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_distribution = Uniform(torch.tensor([3.14- 0.001, -0.01]), torch.tensor([3.14 + 0.001, +0.01]))\n",
    "sparse_reward_model = PendulumSparseReward(action_cost=0.1)\n",
    "dense_reward_model = PendulumDenseReward(action_cost=0.1)\n",
    "environment = SystemEnvironment(\n",
    "        InvertedPendulum(mass=0.3, length=0.5, friction=0.005, step_size=1 / 80),\n",
    "        reward=dense_reward_model,\n",
    "        initial_state=initial_distribution.sample,\n",
    "    )\n",
    "dynamical_model =  PendulumModel(mass=0.3, length=0.5, friction=0.005, step_size=1 / 80)\n",
    "\n",
    "def mpc(sparse_reward, return_to_go, solver, horizon, initial_position):\n",
    "    if sparse_reward:\n",
    "        environment.reward = sparse_reward_model\n",
    "    else:\n",
    "        environment.reward = dense_reward_model\n",
    "       \n",
    "    initial_distribution = Uniform(torch.tensor([initial_position - 0.001, -0.01]), torch.tensor([initial_position + 0.001, +0.01]))\n",
    "    environment.initial_state = initial_distribution.sample\n",
    "    environment.reset()\n",
    "    if return_to_go:\n",
    "        terminal_reward = PendulumReturnToGo()\n",
    "    else:\n",
    "        terminal_reward = None \n",
    "    agent = MPCAgent.default(\n",
    "            environment=environment,\n",
    "            mpc_solver_name=solver,\n",
    "            dynamical_model=dynamical_model,\n",
    "            reward_model=environment.reward,\n",
    "            exploration_episodes=0,\n",
    "            horizon=horizon,\n",
    "            terminal_reward=terminal_reward,\n",
    "        )\n",
    "    try:\n",
    "\n",
    "        rollout_agent(agent=agent, environment=environment, max_steps=400, num_episodes=1, render=True)\n",
    "    except KeyboardInterrupt:\n",
    "        environment.close() \n",
    "\n",
    "interact(\n",
    "    mpc, \n",
    "    action_cost=ipywidgets.FloatSlider(value=0.1, min=0, max=0.5, continuous_update=False),\n",
    "    sparse_reward=False, \n",
    "    return_to_go=False,\n",
    "    solver=[\"CEMShooting\", \"MPPIShooting\", \"RandomShooting\"], \n",
    "    horizon=ipywidgets.IntSlider(value=30, min=5, max=50, continuous_update=False), \n",
    "    initial_position = ipywidgets.FloatSlider(value=np.pi, min=0, max=np.pi, continuous_update=False)    \n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo shows running the model based reinforcement learning method. It means we know the accurate model of the environment.Now the task is to get the optimal policy.\n",
    "- sparse_reward: Using the sparse reward:  A tolerance function is returns 1 if x is in [lower, upper]. If it is outside, it decays exponentially according to a margin. In this pendulum model the low was set to be:\n",
    "$lower=0.95, upper=1.0$ for the cos value of angle (top is 0 angle). and \n",
    "$lower-0.5, upper=0.5$ fpr the velocity.\n",
    "- solver:[CEMShooting(Cross-entropy method)](https://arxiv.org/abs/1907.02057), [Model-Predictive Path Integral(MPPIShooting)](https://ieeexplore.ieee.org/document/7487277), RandomShooting. Different MPC splver.\n",
    "- hoizon: the look ahead horizon of MPC method\n",
    "- initial position: the starting angle of the pendulum model \n",
    "\n",
    "#### play around\n",
    "- Try small horizon and see if the algorithm work or not.\n",
    "- Try small horizon with return_to_go setting to True\n",
    "- Use the sparse reward and compare the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
